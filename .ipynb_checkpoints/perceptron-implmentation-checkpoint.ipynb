{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (1.20.3)\r\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.9/site-packages (0.0)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.2.4)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/site-packages (from sklearn) (0.24.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/site-packages (from pandas) (2021.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.6.3)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.0.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy sklearn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRt0B1K50oYT"
   },
   "source": [
    "# Preceptron Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B3dndmPB0Yp9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, max_iterations=100, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        The constructor method of an object.\n",
    "        \"\"\"\n",
    "        # the perceptron's weights. Initialised by .fit()\n",
    "        self._w = None\n",
    "        self._learning_rate = learning_rate\n",
    "        self._max_iterations = max_iterations\n",
    "\n",
    "    def _add_constant(self, X):\n",
    "        '''Add dummy constant attribute to all samples'''\n",
    "        num_samples = X.shape[0]\n",
    "        X0 = np.transpose([np.ones(num_samples)])\n",
    "        X = np.concatenate([X, X0], axis=1)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def accurancy(self, y, predicted):\n",
    "        '''return the accurancy of a given set of predictions'''\n",
    "        correct_classifications = np.sum(y == predicted)\n",
    "\n",
    "        return correct_classifications / len(y)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        '''predicts y for all samples X'''\n",
    "        # add the constant dummy value to each sample to make\n",
    "        # it possible to perform all calculations using matrix\n",
    "        # algebra\n",
    "        X = self._add_constant(X)\n",
    "        \n",
    "        # perform the prediction and return it\n",
    "        return self._predict(X)\n",
    "\n",
    "    def _predict(self, X):\n",
    "        '''predicts y for all samples X\n",
    "        \n",
    "        assumes that the dummy values have already been applied\n",
    "        '''\n",
    "        # equivalent to applying the weights to each data sample\n",
    "        # and summing\n",
    "        y = np.dot(X, self._w)\n",
    "        \n",
    "        # apply the threshold function\n",
    "        y = np.where(y >= 0.0, 1, -1)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def fit(self, X, y, quiet=True):\n",
    "        \"\"\"\n",
    "        :param X: training data samples\n",
    "        :param y: the target values\n",
    "        :param quiet: if true do not print any output while learning\n",
    "        \"\"\"\n",
    "        # initialise weights based on the number of inputs in X\n",
    "        # this allows us to support data sets with different\n",
    "        # numbers of inputs\n",
    "        self._w = np.zeros(X.shape[1] + 1)\n",
    "        \n",
    "        # add the constant dummy value to each sample to make\n",
    "        # it possible to perform all calculations using matrix\n",
    "        # algebra\n",
    "        X = self._add_constant(X)\n",
    "\n",
    "        # counter used to track the number of iterations. We\n",
    "        # stop the learning when max_iterations is reached.\n",
    "        iterations = 0\n",
    "\n",
    "        while True:\n",
    "            # predict using current weights\n",
    "            predicted = self._predict(X)\n",
    "\n",
    "            # count the number of errors for using the current weights\n",
    "            # to predict the target values y for all samples X\n",
    "            errors = 0\n",
    "            \n",
    "            # loop through each data sample, its target and our prediction\n",
    "            for xi, yi, predi in zip(X, y, predicted):\n",
    "                # calculate the update to make based on the difference between\n",
    "                # target and prediction. Multiply by the learning rate to\n",
    "                # improve convergence.\n",
    "                update = self._learning_rate * (yi - predi)\n",
    "                \n",
    "                # update the weights\n",
    "                self._w += update * xi\n",
    "                \n",
    "                # store the number of errors for this sample\n",
    "                errors += int(update != 0.0)\n",
    "            \n",
    "            # produce some output to show how low the learning process is\n",
    "            # progressing\n",
    "            accurancy = self.accurancy(y, predicted)\n",
    "            if not quiet:\n",
    "              print(f'iterations: {iterations} weights: {self._w} ' +\n",
    "                    f'errors: {errors} accuracy: {accurancy:.2f}')\n",
    "\n",
    "            # increment the iteration count and stop learning if we've reached\n",
    "            # max_iterations.\n",
    "            iterations += 1\n",
    "            if iterations > self._max_iterations:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqDb0V9x0_Ds"
   },
   "source": [
    "# Data Set 1: Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODt_EIu4bb4T"
   },
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HfQib9Go1D5v"
   },
   "outputs": [],
   "source": [
    "# imports required for the iris data set and data set manipulation\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The Iris dataset will be used to demonstrate that the\n",
    "# perceptron works with 2 classes and 5 inputs.\n",
    "\n",
    "# load the iris data set\n",
    "iris = load_iris()\n",
    "X, y = iris['data'], iris['target']\n",
    "\n",
    "# Reduce the classification from ['setosa', 'versicolor', 'virginica']\n",
    "# to ['not virginica', 'virginica']\n",
    "two_class_index = np.logical_or(y == 0, y == 1)\n",
    "simple_X = X[two_class_index]\n",
    "simple_y = y[two_class_index]\n",
    "\n",
    "# It is more convenient for the implementation if the output is 1 or -1 rather\n",
    "# than 1 or 0. Convert all the 0's to -1's.\n",
    "simple_y[simple_y==0] = -1\n",
    "\n",
    "# Split that data into training and testing sets\n",
    "simple_X_train, simple_X_valid, simple_y_train, simple_y_valid = \\\n",
    "    train_test_split(simple_X, simple_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWSAIECl2zYu"
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9QKLQPu1eg5",
    "outputId": "cdbeb2ab-c7f1-4c73-88f1-929d97096505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations: 0 weights: [-0.3316 -0.2284 -0.0972 -0.0164 -0.066 ] errors: 33 accuracy: 0.56\n",
      "iterations: 1 weights: [0.1698 0.0066 0.2622 0.0962 0.018 ] errors: 42 accuracy: 0.44\n",
      "iterations: 2 weights: [-0.1618 -0.2218  0.165   0.0798 -0.048 ] errors: 33 accuracy: 0.56\n",
      "iterations: 3 weights: [0.3396 0.0132 0.5244 0.1924 0.036 ] errors: 42 accuracy: 0.44\n",
      "iterations: 4 weights: [ 0.008  -0.2152  0.4272  0.176  -0.03  ] errors: 33 accuracy: 0.56\n",
      "iterations: 5 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 8 accuracy: 0.89\n",
      "iterations: 6 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 0 accuracy: 1.00\n",
      "iterations: 7 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 0 accuracy: 1.00\n",
      "iterations: 8 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 0 accuracy: 1.00\n",
      "iterations: 9 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 0 accuracy: 1.00\n",
      "iterations: 10 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 0 accuracy: 1.00\n",
      "iterations: 11 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 0 accuracy: 1.00\n",
      "iterations: 12 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 0 accuracy: 1.00\n",
      "iterations: 13 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 0 accuracy: 1.00\n",
      "iterations: 14 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 0 accuracy: 1.00\n",
      "iterations: 15 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 0 accuracy: 1.00\n",
      "iterations: 16 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 0 accuracy: 1.00\n",
      "iterations: 17 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 0 accuracy: 1.00\n",
      "iterations: 18 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 0 accuracy: 1.00\n",
      "iterations: 19 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 0 accuracy: 1.00\n",
      "iterations: 20 weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ] errors: 0 accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# create the Perceptron model\n",
    "# the default options are:\n",
    "# - max_iterations = 100\n",
    "# - learning_rate  = 0.001\n",
    "iris_model = Perceptron(max_iterations=20)\n",
    "\n",
    "# train the model\n",
    "# the default options are:\n",
    "# - quiet = True, does not produce any output while training\n",
    "iris_model.fit(simple_X_train, simple_y_train, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ke5gaTj030xu"
   },
   "source": [
    "## Validate The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUWsY5At1-Ov",
    "outputId": "e0884ee3-6cd1-4045-b9c0-4336d854bba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Model weights: [-0.0706 -0.2686  0.4006  0.1708 -0.046 ]\n",
      "Accuracy of the predictor on the *training* data set: 1.00\n",
      "Accuracy of the predictor on the *test* data set: 1.00\n"
     ]
    }
   ],
   "source": [
    "print(f'Iris Model weights: {iris_model._w}')\n",
    "pred_train = iris_model.predict(simple_X_train)\n",
    "training_accuracy = iris_model.accurancy(simple_y_train, pred_train)\n",
    "print(f'Accuracy of the predictor on the *training* data set: {training_accuracy:.2f}')\n",
    "\n",
    "pred_valid = iris_model.predict(simple_X_valid)\n",
    "validation_accuracy = iris_model.accurancy(simple_y_valid, pred_valid)\n",
    "print(f'Accuracy of the predictor on the *test* data set: {validation_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRWr2aFsFFIe"
   },
   "source": [
    "# Data Set 2: Australian Weather data\n",
    "\n",
    "This data set includes various weather metrics for austrailian cities. The metric \"RainTomorrow\" represents whether there will be\n",
    "rain the following day and is used as the target. We will use this data set to train a classifier to predict whether there will be rain tomorrow.\n",
    "\n",
    "Data set used: https://www.kaggle.com/jsphyg/weather-dataset-rattle-package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pj_dlbYLuWW"
   },
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AUaoCmW539EU",
    "outputId": "9c8bba34-4a9a-4490-ee46-08de772f0ab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples is 1898\n",
      "Number of samples with a target value of -1 is 1427\n",
      "Number of samples with a target value of 1 is 471\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the data file from the repository that contains this notebook\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/davidsackett/perceptron-implementation/master/weatherAUS.csv')\n",
    "\n",
    "# the following line can be used to see all the cities in the data set and\n",
    "# now many samples are available for each city.\n",
    "# print(data['Location'].value_counts())\n",
    "\n",
    "# filter down to a single city. In this case Melbourne\n",
    "cityFilter = data['Location'] == 'Melbourne'\n",
    "\n",
    "# the following line can be used to see how many samples we have\n",
    "# what the available columns are, their types and how may non-null\n",
    "# values\n",
    "# print(cityData.info())\n",
    "\n",
    "# remove any samples with missing data. I.e. NaN\n",
    "cityData = data[cityFilter].dropna()\n",
    "\n",
    "# select some interesting columns that are floats and may help\n",
    "# us predict the target\n",
    "simple_X = cityData[['MinTemp',\n",
    "                     'MaxTemp',\n",
    "                     'Rainfall',\n",
    "                     'Pressure9am',\n",
    "                     'Pressure3pm',\n",
    "                     'Temp9am',\n",
    "                     'Temp3pm']].to_numpy()\n",
    "\n",
    "# use the rain tomorrow column as our target\n",
    "# convert the Yes value to 1 or 0\n",
    "simple_y = (cityData['RainTomorrow'].to_numpy()=='Yes').astype(int)\n",
    "\n",
    "# convert 1/0 to 1/-1\n",
    "simple_y[simple_y==0] = -1\n",
    "\n",
    "# split the data into training set and test set\n",
    "simple_X_train, simple_X_valid, simple_y_train, simple_y_valid = \\\n",
    "    train_test_split(simple_X, simple_y, random_state=42)\n",
    "\n",
    "# count the number of unique targets in our data set\n",
    "unique_values = np.unique(simple_y, return_counts=True)\n",
    "print(f'Number of samples is {len(simple_y)}')\n",
    "for target_value, count in zip(unique_values[0], unique_values[1]):\n",
    "    print(f'Number of samples with a target value of {target_value} is {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_OdOT6HLzNP"
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "alGj_5z0FZhS"
   },
   "outputs": [],
   "source": [
    "weather_model = Perceptron(max_iterations=1000, learning_rate=0.01) # initiate an object\n",
    "weather_model.fit(simple_X_train, simple_y_train, quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uF3QgDlvbqtr"
   },
   "source": [
    "## Validate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IyxWfB3XIpfw",
    "outputId": "394e3dc9-a565-4947-fd64-cd6dee0879e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather Model weights [  2699.246 -10582.494   9072.164  -3084.298  -1036.012    911.458\n",
      " -12931.32      34.74 ]\n",
      "Accuracy of the predictor on the training data set: 0.73\n",
      "Accuracy of the predictor on the test data set: 0.80\n"
     ]
    }
   ],
   "source": [
    "print('Weather Model weights', weather_model._w)\n",
    "pred_train = weather_model.predict(simple_X_train)\n",
    "training_accuracy = weather_model.accurancy(simple_y_train, pred_train)\n",
    "print(f'Accuracy of the predictor on the training data set: {training_accuracy:.2f}')\n",
    "\n",
    "pred_valid = weather_model.predict(simple_X_valid)\n",
    "validation_accuracy = weather_model.accurancy(simple_y_valid, pred_valid)\n",
    "print(f'Accuracy of the predictor on the test data set: {validation_accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Machine Learning Fundamentals Assignment - David Sackett.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
